{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Imoprt Dependencies\n",
    "\n",
    "pip install stable-baselines3[extra]\n",
    "\n",
    "https://stable-baselines3.readthedocs.io/en/master/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os                                                           # operating system library\n",
    "import gymnasium as gym                                             # for openAI gymnasium (which has replaced openai gym)\n",
    "from stable_baselines3 import PPO                                   # algorithm\n",
    "# Action Spaces: Algorithms\n",
    "#   Discrete Single Process: DQN\n",
    "#   Discrete Multi Professed: PPO or A2C\n",
    "#   Continuous Single Process: SAC or TD3\n",
    "#   Continuous Multi Processed: PPO or A2C\n",
    "\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv            # vectorizing environment allows train on multiple environments at the same time (this doesn't do that)\n",
    "from stable_baselines3.common.evaluation import evaluate_policy     # mean and stdev of reward\n",
    "\n",
    "# CALLBACKS\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Load Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment name\n",
    "environment_name = 'CartPole-v1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CartPole-v1\n"
     ]
    }
   ],
   "source": [
    "# create the environment\n",
    "env = gym.make(environment_name,render_mode = 'human')\n",
    "\n",
    "print(environment_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(2)\n",
      "0\n",
      "Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n",
      "[ 2.9300828e+00  1.5114768e+38  1.5634502e-01 -2.3155817e+38]\n",
      "(array([ 0.04802989,  0.19765113, -0.04387073, -0.3552087 ], dtype=float32), 1.0, False, False, {})\n"
     ]
    }
   ],
   "source": [
    "# two different spaces:\n",
    "# action space (actions we can do)\n",
    "print(env.action_space)\n",
    "print(env.action_space.sample())\n",
    "\n",
    "# and observations (things we observe)\n",
    "print(env.observation_space)\n",
    "print(env.observation_space.sample())\n",
    "\n",
    "state = env.reset()\n",
    "print(env.step(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:13.0\n",
      "Episode:2 Score:16.0\n",
      "Episode:3 Score:13.0\n",
      "Episode:4 Score:32.0\n",
      "Episode:5 Score:56.0\n"
     ]
    }
   ],
   "source": [
    "# loop for testing environment\n",
    "episodes = 5\n",
    "for episode in range(1,episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    while not done:\n",
    "        # make the render\n",
    "        env.render()\n",
    "\n",
    "        # select the next action\n",
    "        action = env.action_space.sample()\n",
    "\n",
    "        # take a step using the action and return the new state, reward, is the episode done? x2, info\n",
    "        n_state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = truncated or terminated\n",
    "\n",
    "        \n",
    "\n",
    "        # aggregate the reward\n",
    "        score += reward\n",
    "\n",
    "    print('Episode:{} Score:{}'.format(episode, score))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Train an RL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Logging to Training\\Logs\\PPO_1\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 929  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 2    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 774          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 5            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064420556 |\n",
      "|    clip_fraction        | 0.083        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.687       |\n",
      "|    explained_variance   | -0.00295     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.72         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.0115      |\n",
      "|    value_loss           | 54.8         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 750         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010390015 |\n",
      "|    clip_fraction        | 0.074       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.668      |\n",
      "|    explained_variance   | 0.0686      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 15.8        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0193     |\n",
      "|    value_loss           | 38.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 667         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009927827 |\n",
      "|    clip_fraction        | 0.123       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.628      |\n",
      "|    explained_variance   | 0.321       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 17.5        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0254     |\n",
      "|    value_loss           | 47.1        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 466          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 21           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068558566 |\n",
      "|    clip_fraction        | 0.045        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.615       |\n",
      "|    explained_variance   | 0.143        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 21.3         |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.0104      |\n",
      "|    value_loss           | 67.9         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 386          |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 31           |\n",
      "|    total_timesteps      | 12288        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071811015 |\n",
      "|    clip_fraction        | 0.0792       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.589       |\n",
      "|    explained_variance   | 0.424        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 27.6         |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.0139      |\n",
      "|    value_loss           | 63.9         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 344          |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 41           |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0088363495 |\n",
      "|    clip_fraction        | 0.0906       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.587       |\n",
      "|    explained_variance   | 0.368        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 28.8         |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.0118      |\n",
      "|    value_loss           | 69.1         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 315         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 51          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007892262 |\n",
      "|    clip_fraction        | 0.0792      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.58       |\n",
      "|    explained_variance   | 0.668       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 20.7        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0127     |\n",
      "|    value_loss           | 57.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 295         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 62          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005124986 |\n",
      "|    clip_fraction        | 0.039       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.565      |\n",
      "|    explained_variance   | 0.616       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 14.6        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00761    |\n",
      "|    value_loss           | 47.7        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 280        |\n",
      "|    iterations           | 10         |\n",
      "|    time_elapsed         | 72         |\n",
      "|    total_timesteps      | 20480      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00457258 |\n",
      "|    clip_fraction        | 0.0218     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.551     |\n",
      "|    explained_variance   | 0.593      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 18.7       |\n",
      "|    n_updates            | 90         |\n",
      "|    policy_gradient_loss | -0.00454   |\n",
      "|    value_loss           | 59.1       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x1c4e5897f10>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# where we save our tensorboard log\n",
    "log_path = os.path.join('Training','Logs')\n",
    "\n",
    "# recreate the environment\n",
    "environment_name = 'CartPole-v1'\n",
    "env = gym.make(environment_name)\n",
    "\n",
    "# wrap environ in dummy vec environment (wrapper for non-vectorized environment)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "\n",
    "# create the model: multilayer neural network policy (the rules teh agent uses to determine its actions)\n",
    "model = PPO('MlpPolicy',env,verbose=1,tensorboard_log=log_path)\n",
    "\n",
    "# train model (including number of timesteps to train model for)\n",
    "model.learn(total_timesteps=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Save and Reload Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to save model to\n",
    "PPO_Path = os.path.join('Training','Saved Models','PPO_Model_Cartpole')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test saving an deleting the model\n",
    "model.save(PPO_Path)\n",
    "del model\n",
    "model = PPO.load(PPO_Path,env=env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Joe\\Documents\\MachineLearningProjects\\StableBaselinesTutorials\\.venv\\Lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# score of 200 or higher is generally considered solved for PPO\n",
    "\n",
    "# load model\n",
    "env = gym.make(environment_name,render_mode = 'human')\n",
    "model = PPO.load(PPO_Path,env=env)\n",
    "\n",
    "# returns mean and std of rewards\n",
    "evaluate_policy(model,env,n_eval_episodes=10,render=True)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Episode:1 Score:403.0\n",
      "Episode:2 Score:292.0\n",
      "Episode:3 Score:500.0\n",
      "Episode:4 Score:371.0\n",
      "Episode:5 Score:375.0\n"
     ]
    }
   ],
   "source": [
    "# take observations, pass them to agent, agent will determine action using learned policy\n",
    "# - in this example the agent is the model object and the environment is the env object\n",
    "# - the actions and the observations/rewards are things that are returned from the agent and envirnment respectively\n",
    "# - there are a variety of methods that can be used to return an action or an observation/rewards\n",
    "\n",
    "# load model\n",
    "env = gym.make(environment_name,render_mode = 'human')\n",
    "model = PPO.load(PPO_Path,env=env)\n",
    "\n",
    "# loop for testing environment\n",
    "episodes = 5\n",
    "for episode in range(1,episodes+1):\n",
    "    obs = env.reset()[0]\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    while not done:\n",
    "        # make the render\n",
    "        env.render()\n",
    "\n",
    "        # NOW USING MODEL HERE!!\n",
    "        action, _ = model.predict(obs)\n",
    "\n",
    "        # take a step using the action and return the new state, reward, is the episode done? x2, info\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        done = truncated or terminated\n",
    "\n",
    "        # aggregate the reward\n",
    "        score += reward\n",
    "\n",
    "    print('Episode:{} Score:{}'.format(episode, score))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Viewing Logs in Tensorboard\n",
    "\n",
    "Ideally want to do this in terminal/cmd line so that we don't bog down the notebook\n",
    "Steps:\n",
    "1) specify path to folder with log\n",
    "\n",
    "    ipynb: training_log_path = os.path.join(log_path,PPO_2)\n",
    "    \n",
    "    cmd: ./Training/Logs/PPO_2\n",
    "2) launch tensorboard\n",
    "\n",
    "    ipynb: !tensorboard --logdir training_log_path\n",
    "\n",
    "    cmd: tensorboard --logdir ./Training/Logs/PPO_#\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Adding a Callback to the Training Stage\n",
    "\n",
    "    Now we will apply callbacks (we will stop training once we reach a desired performance, etc. and save the model at our desired condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to Training\\Logs\\PPO_2\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 23       |\n",
      "|    ep_rew_mean     | 23       |\n",
      "| time/              |          |\n",
      "|    fps             | 46       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 43       |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 29.8        |\n",
      "|    ep_rew_mean          | 29.8        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 45          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 89          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009020019 |\n",
      "|    clip_fraction        | 0.112       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.686      |\n",
      "|    explained_variance   | 5.53e-05    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.51        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0181     |\n",
      "|    value_loss           | 52.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 38          |\n",
      "|    ep_rew_mean          | 38          |\n",
      "| time/                   |             |\n",
      "|    fps                  | 45          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 134         |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008497771 |\n",
      "|    clip_fraction        | 0.0576      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.667      |\n",
      "|    explained_variance   | 0.057       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 17.5        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0172     |\n",
      "|    value_loss           | 44.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 44.9        |\n",
      "|    ep_rew_mean          | 44.9        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 45          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 180         |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008024545 |\n",
      "|    clip_fraction        | 0.0814      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.635      |\n",
      "|    explained_variance   | 0.168       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 25.1        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0189     |\n",
      "|    value_loss           | 53.5        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Joe\\Documents\\MachineLearningProjects\\StableBaselinesTutorials\\.venv\\Lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=10000, episode_reward=432.80 +/- 88.17\n",
      "Episode length: 432.80 +/- 88.17\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 433         |\n",
      "|    mean_reward          | 433         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011458362 |\n",
      "|    clip_fraction        | 0.103       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.602      |\n",
      "|    explained_variance   | 0.251       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 33.1        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0188     |\n",
      "|    value_loss           | 72          |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Stopping training because the mean reward 432.80  is above the threshold 200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x1c4e5872090>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_path = os.path.join('Training','Saved Models')\n",
    "env = gym.make(environment_name,render_mode = 'human')\n",
    "log_path = os.path.join('Training','Logs')\n",
    "\n",
    "# this is the callback that will stop our training once we pass a certain reward threshold (the average reward we want to stop on and verbose to get some additional logging)\n",
    "stop_callback = StopTrainingOnRewardThreshold(reward_threshold=200, verbose=1)  \n",
    "# the callback that will be triggered after each training run\n",
    "# pass the:\n",
    "#   - environment\n",
    "#   - the callback we want to run on the new best model: every time ther eis a new bet model it will run the stop callback and check the reward threshold\n",
    "#   - how frequently we want to run the eval callback\n",
    "#   - path to the model\n",
    "# basically every 10000 runs it will check the reward threshold, if it's above it will stop the training and save the model\n",
    "eval_callback = EvalCallback(env,\n",
    "                             callback_on_new_best=stop_callback,\n",
    "                             eval_freq=10000,\n",
    "                             best_model_save_path=save_path,\n",
    "                             verbose=1)\n",
    "\n",
    "# need to associate these callbacks with our model              \n",
    "model = PPO('MlpPolicy', env, verbose=1, tensorboard_log=log_path)\n",
    "\n",
    "# now will pass callback in when we train our model\n",
    "model.learn(total_timesteps=20000, callback=eval_callback)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Changing Policies\n",
    "\n",
    "    We can change our model architecture if we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Joe\\Documents\\MachineLearningProjects\\StableBaselinesTutorials\\.venv\\Lib\\site-packages\\stable_baselines3\\common\\policies.py:486: UserWarning: As shared layers in the mlp_extractor are removed since SB3 v1.8.0, you should now pass directly a dictionary and not a list (net_arch=dict(pi=..., vf=...) instead of net_arch=[dict(pi=..., vf=...)])\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training\\Logs\\PPO_3\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 22.6     |\n",
      "|    ep_rew_mean     | 22.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 42       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 47       |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 29.4        |\n",
      "|    ep_rew_mean          | 29.4        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 43          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 93          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014564334 |\n",
      "|    clip_fraction        | 0.235       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.681      |\n",
      "|    explained_variance   | 0.00396     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.39        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0288     |\n",
      "|    value_loss           | 20.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 41.5        |\n",
      "|    ep_rew_mean          | 41.5        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 44          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 138         |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015089423 |\n",
      "|    clip_fraction        | 0.204       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.641      |\n",
      "|    explained_variance   | 0.451       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11.2        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0357     |\n",
      "|    value_loss           | 25.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 56.2        |\n",
      "|    ep_rew_mean          | 56.2        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 44          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 183         |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014329901 |\n",
      "|    clip_fraction        | 0.166       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.596      |\n",
      "|    explained_variance   | 0.46        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 16.8        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0292     |\n",
      "|    value_loss           | 40.2        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Joe\\Documents\\MachineLearningProjects\\StableBaselinesTutorials\\.venv\\Lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=10000, episode_reward=459.80 +/- 50.46\n",
      "Episode length: 459.80 +/- 50.46\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 460         |\n",
      "|    mean_reward          | 460         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009463313 |\n",
      "|    clip_fraction        | 0.139       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.584      |\n",
      "|    explained_variance   | 0.469       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 16.3        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.02       |\n",
      "|    value_loss           | 44.4        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Stopping training because the mean reward 459.80  is above the threshold 200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x21d85e95690>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_path = os.path.join('Training','Saved Models')\n",
    "environment_name = 'CartPole-v1'\n",
    "env = gym.make(environment_name,render_mode = 'human')\n",
    "log_path = os.path.join('Training','Logs')\n",
    "\n",
    "# this is the callback that will stop our training once we pass a certain reward threshold (the average reward we want to stop on and verbose to get some additional logging)\n",
    "stop_callback = StopTrainingOnRewardThreshold(reward_threshold=200, verbose=1)  \n",
    "# the callback that will be triggered after each training run\n",
    "# pass the:\n",
    "#   - environment\n",
    "#   - the callback we want to run on the new best model: every time ther eis a new bet model it will run the stop callback and check the reward threshold\n",
    "#   - how frequently we want to run the eval callback\n",
    "#   - path to the model\n",
    "# basically every 10000 runs it will check the reward threshold, if it's above it will stop the training and save the model\n",
    "eval_callback = EvalCallback(env,\n",
    "                             callback_on_new_best=stop_callback,\n",
    "                             eval_freq=10000,\n",
    "                             best_model_save_path=save_path,\n",
    "                             verbose=1)\n",
    "\n",
    "# specify a custom neural network architecture\n",
    "net_arch = [dict(pi=[128,128,128,128], vf=[128,128,128,128])]\n",
    "\n",
    "# declare model and train with new model\n",
    "model = PPO('MlpPolicy', env, verbose=1, tensorboard_log=log_path, policy_kwargs={'net_arch':net_arch})\n",
    "model.learn(total_timesteps=20000, callback=eval_callback)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Using an Alternate Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import DQN\n",
    "from stable_baselines3 import DQN\n",
    "\n",
    "# create the model\n",
    "model = DQN('MlpPolicy', env, verbose=1, tensorboard_log=log_path)\n",
    "\n",
    "# train the model\n",
    "model.learn(total_timesteps=20000)\n",
    "\n",
    "# to load the model use DQN.load instead of PPO.load\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
